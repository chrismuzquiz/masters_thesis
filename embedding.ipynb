{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"affectivetext_data.csv\")\n",
    "labels = [\"anger\", \"disgust\", \"fear\", \"joy\", \"sadness\", \"surprise\", \"valence\"]\n",
    "df = df.reindex(columns=[\"headline\", *labels])\n",
    "\n",
    "# note that quotation marks around the entire string are lost in `DataFrame.read_csv`\n",
    "# Instances of double double quotes (\"\"_\"\") are reduced to a single pair of double quotes (\"_\")\n",
    "\n",
    "# df.ndim = 2\n",
    "# df.shape = (1250, 8)\n",
    "n = df.shape[0] # = 1250 data points in total\n",
    "\n",
    "df.iloc[[0,1,34,44,248,292,308,981,998,999,1000,1001,1248,1249],:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "                                           headline  anger  disgust  fear  \\\n0            Mortar assault leaves at least 18 dead     22        2    60   \n1                            Goal delight for Sheva      0        0     0   \n34                 The sweet tune of an anniversary      0        0     0   \n44              US Troops Killed In October In Iraq     46       39    41   \n248               Man admits UK-US terror bomb plot     33       43    61   \n292                Federer handed tough Aussie draw      0        0     0   \n308              Ganguly handed India squad call-up      4        4     6   \n981             Toyota's Scion parks in Second Life      0        0     0   \n998   Weekly Nielsen will 'Lost' find its way back?      0        0    11   \n999   Brith Airways baggage charges cause confusion      5       13     4   \n1000                  Indonesian with bird flu dies      0        0    45   \n1001     St. Louis poised for entertainment upswing      0        0     0   \n1248               Study links chimps and 'hammers'      0        0     0   \n1249             UN-Google Earth map climate change     10        6    14   \n\n      joy  sadness  surprise  valence  \n0       0       64         0      -98  \n1      93        0        38       87  \n34     83        0         0       65  \n44      0       77         0      -79  \n248     0       33         0      -55  \n292     4        8         4        4  \n308    31        5        12        6  \n981    17        4        18       23  \n998    17        9        11       21  \n999     0       14        18      -36  \n1000    0       73        21      -86  \n1001   54        0        15       69  \n1248   14        0        43       19  \n1249   41       17        31       44  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>headline</th>\n      <th>anger</th>\n      <th>disgust</th>\n      <th>fear</th>\n      <th>joy</th>\n      <th>sadness</th>\n      <th>surprise</th>\n      <th>valence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Mortar assault leaves at least 18 dead</td>\n      <td>22</td>\n      <td>2</td>\n      <td>60</td>\n      <td>0</td>\n      <td>64</td>\n      <td>0</td>\n      <td>-98</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Goal delight for Sheva</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>93</td>\n      <td>0</td>\n      <td>38</td>\n      <td>87</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>The sweet tune of an anniversary</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>83</td>\n      <td>0</td>\n      <td>0</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>US Troops Killed In October In Iraq</td>\n      <td>46</td>\n      <td>39</td>\n      <td>41</td>\n      <td>0</td>\n      <td>77</td>\n      <td>0</td>\n      <td>-79</td>\n    </tr>\n    <tr>\n      <th>248</th>\n      <td>Man admits UK-US terror bomb plot</td>\n      <td>33</td>\n      <td>43</td>\n      <td>61</td>\n      <td>0</td>\n      <td>33</td>\n      <td>0</td>\n      <td>-55</td>\n    </tr>\n    <tr>\n      <th>292</th>\n      <td>Federer handed tough Aussie draw</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>8</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>308</th>\n      <td>Ganguly handed India squad call-up</td>\n      <td>4</td>\n      <td>4</td>\n      <td>6</td>\n      <td>31</td>\n      <td>5</td>\n      <td>12</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>981</th>\n      <td>Toyota's Scion parks in Second Life</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>17</td>\n      <td>4</td>\n      <td>18</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>Weekly Nielsen will 'Lost' find its way back?</td>\n      <td>0</td>\n      <td>0</td>\n      <td>11</td>\n      <td>17</td>\n      <td>9</td>\n      <td>11</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>Brith Airways baggage charges cause confusion</td>\n      <td>5</td>\n      <td>13</td>\n      <td>4</td>\n      <td>0</td>\n      <td>14</td>\n      <td>18</td>\n      <td>-36</td>\n    </tr>\n    <tr>\n      <th>1000</th>\n      <td>Indonesian with bird flu dies</td>\n      <td>0</td>\n      <td>0</td>\n      <td>45</td>\n      <td>0</td>\n      <td>73</td>\n      <td>21</td>\n      <td>-86</td>\n    </tr>\n    <tr>\n      <th>1001</th>\n      <td>St. Louis poised for entertainment upswing</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>54</td>\n      <td>0</td>\n      <td>15</td>\n      <td>69</td>\n    </tr>\n    <tr>\n      <th>1248</th>\n      <td>Study links chimps and 'hammers'</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>14</td>\n      <td>0</td>\n      <td>43</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>1249</th>\n      <td>UN-Google Earth map climate change</td>\n      <td>10</td>\n      <td>6</td>\n      <td>14</td>\n      <td>41</td>\n      <td>17</td>\n      <td>31</td>\n      <td>44</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import string\n",
    "\n",
    "headlines = pd.Series.tolist(df[\"headline\"])\n",
    "\n",
    "tabs = [headline.count('\\t') for headline in headlines]\n",
    "# print([i for i, e in enumerate(tabs) if e != 0])\n",
    "\n",
    "def surr_quotes(s):\n",
    "    if s[0] == \"'\" and s[-1] == \"'\":\n",
    "        return 1\n",
    "    return 0\n",
    "quoted = [surr_quotes(headline) for headline in headlines]\n",
    "# print([i for i, e in enumerate(quoted) if e != 0])\n",
    "\n",
    "# replace any tab characters with spaces\n",
    "headlines = [headline.replace('\\t',' ') for headline in headlines]\n",
    "# replace single instance of grave accent (used incorrectly) with single quote\n",
    "headlines = [headline.replace(\"`\",\"'\") for headline in headlines]\n",
    "# remove single quote surrounding headlines[248] (this is the only instance of a single-quoted headline)\n",
    "headlines[248] = headlines[248][1:-1]\n",
    "# downcase letters\n",
    "headlines1 = [headline.lower() for headline in headlines]\n",
    "\n",
    "# list of all characters in headlines ordered by ascii value\n",
    "char_list = list(set(\"\".join(headlines1)))\n",
    "char_list.sort()\n",
    "# number of distinct characters in the headlines\n",
    "num_distinct_chars = len(char_list)\n",
    "# the maximum number of characters in headlines\n",
    "max_chars = max([len(headline) for headline in headlines1])\n",
    "\n",
    "print(f\"{char_list = }\\n{num_distinct_chars = }\\n{max_chars = }\")\n",
    "\n",
    "print()\n",
    "\n",
    "# replace any hyphens with spaces so that less information is lost\n",
    "headlines2 = [headline.replace('-',' ') for headline in headlines1]\n",
    "# obtain list of list of words in each headline\n",
    "headlines_words = [headline.split() for headline in headlines2]\n",
    "# rejoin word lists to remove extraneous spaces\n",
    "headlines2 = [' '.join(headline) for headline in headlines_words]\n",
    "# remove punctuation from headlines\n",
    "exclude = string.punctuation\n",
    "headlines2 = [''.join(ch for ch in headline if ch not in exclude) for headline in headlines2]\n",
    "char_list2 = list(set(\"\".join(headlines2)))\n",
    "char_list2.sort()\n",
    "# obtain list of list of words in each headline\n",
    "headlines_words = [headline.split() for headline in headlines2]\n",
    "# number of distinct words in the headlines\n",
    "num_distinct_words = len(set([word for words in headlines_words for word in words]))\n",
    "# the maximum number of words in headlines\n",
    "max_words = max([len(headline) for headline in headlines_words])\n",
    "words_per_headline = np.array([len(words) for words in headlines_words])\n",
    "mean_words_per_headline = np.mean(words_per_headline)\n",
    "\n",
    "print(f\"{char_list2 = }\\n{num_distinct_words = }\\n{max_words = }\\n{mean_words_per_headline = }\")\n",
    "np.quantile(words_per_headline,.25), np.quantile(words_per_headline,.5), np.quantile(words_per_headline,.75)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char_list = [' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '+', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|']\n",
      "num_distinct_chars = 53\n",
      "max_chars = 88\n",
      "\n",
      "char_list2 = [' ', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "num_distinct_words = 3438\n",
      "max_words = 15\n",
      "mean_words_per_headline = 6.5696\n"
     ]
    },
    {
     "data": {
      "text/plain": "(5.0, 6.0, 8.0)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<pre>\n",
    "X0 (1) = Index encoding.<br>\n",
    "    Integer encoding of headlines from the dataset. This does not inherently contain\n",
    "    about the headlines. This will be used with the string kernel so that GloVe\n",
    "    embeddings may be referenced instead of created.\n",
    "\n",
    "X1 (88) = Character-level integer encoding.<br>\n",
    "    Characters embedded are spaces (\" \"), digits (1-10), and lowercase English alphabet (a-z).\n",
    "    The number of distinct characters encoded in X1 is 37. Each integer encoding has a\n",
    "    corresponding one-hot encoding: 0 = [1,0,0,...,0], 1 = [0,1,0,...,0], 2 = [0,0,1,...,0],\n",
    "    and so on. The maximum number of characters in a headline is 85 (including spaces),\n",
    "    so for all headlines with less than the maximum, the encoding is padded with 0s.\n",
    "        Uses `headlines1`\n",
    "\n",
    "X2 (15) = Word-level integer encoding.<br>\n",
    "    This uses the same coding scheme as above, but now there are 3438 distinct words being\n",
    "    encoded. This coding scheme was done using `keras_preprocessing` `text.hashing_trick`\n",
    "    with arguments of 5000 for the hash vocabulary size and md5 as the selected hash function.\n",
    "        Uses `headlines2`\n",
    "\n",
    "X3 (50) = Averaged GloVe embeddings.<br>\n",
    "    The emebeddings used are the 50-dimensional version. The tokens are generated using the\n",
    "    `nltk.tokenize` `word_tokenize` method. In this embedding, punctuation was retained because\n",
    "    embeddings exist for tokenized punctuation.\n",
    "        Uses `headlines1`\n",
    "\n",
    "X4 = (6) Doc2Vec embeddings using the distributed memory training algorithm.<br>\n",
    "        Uses `headlines2`\n",
    "\n",
    "X5 = (6) Doc2Vec embeddings using the distributed bag of words training algorithm.<br>\n",
    "        Uses `headlines2`\n",
    "\n",
    "X = torch.cat((X0,X1,X2,X3,X4,X5),1)\n",
    "\n",
    "X_i     size    index\n",
    "0       1       0\n",
    "1       88      1-88\n",
    "2       15      89-103\n",
    "3       50      104-153\n",
    "4       6       154-159\n",
    "5       6       160-165\n",
    "</pre>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "X0 = torch.arange(n).unsqueeze(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created character-level integer encodings\n"
     ]
    }
   ],
   "source": [
    "X1 = torch.zeros(n,max_chars)\n",
    "\n",
    "char_encoding = dict([(key, value) for value, key in enumerate(char_list)])\n",
    "\n",
    "for i,headline in enumerate(headlines1):\n",
    "    num_chars = len(headline)\n",
    "    X1[i,0:num_chars] = torch.tensor([char_encoding[ch] for ch in headline])\n",
    "\n",
    "print(\"Created character-level integer encodings\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "6.5696"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_encoding\n",
    "# set([len(headline) for headline in headlines_words])\n",
    "sum([len(headline) for headline in headlines_words])/len([len(headline) for headline in headlines_words])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created word-level integer encodings\n"
     ]
    }
   ],
   "source": [
    "from keras_preprocessing import text\n",
    "\n",
    "X2 = torch.zeros(n,max_words)\n",
    "\n",
    "hash_vocab_size = 5000\n",
    "\n",
    "# text.one_hot(s1, 1000), text.hashing_trick(s1,1000,hash_function='md5')\n",
    "# one_hot is wrapper function to hashing_trick function. one_hot uses default python 'hash' function\n",
    "# 'md5' has function is a stable hashing function, while 'hash' is not\n",
    "# vocab size determines the number of unique hash values. Number of unique hashes is vocab size minus 1 since\n",
    "# `0` is a reserved index that won't be assigned to any word.\n",
    "# https://github.com/keras-team/keras-preprocessing/blob/1.1.2/keras_preprocessing/text.py#L253-L267\n",
    "\n",
    "for i, headline in enumerate(headlines2):\n",
    "    num_words = len(headlines_words[i])\n",
    "    X2[i,0:num_words] = torch.tensor(text.hashing_trick(headline, hash_vocab_size, hash_function='md5'))\n",
    "\n",
    "print(\"Created word-level integer encodings\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "glove_dim = 50\n",
    "\n",
    "X3 = torch.zeros(n,glove_dim)\n",
    "word_level_glove_embs = []\n",
    "\n",
    "from embedding_loading import EmbeddingLoading\n",
    "\n",
    "# choose glove dimensions from (50, 100, 200, 300) and call in list.\n",
    "# For example,\n",
    "# glove_dim = [50, 100, 200, 300]\n",
    "# [glove50, glove100, glove200, glove300] = EmbeddingLoading.load(glove_dim)\n",
    "\n",
    "[glove] = EmbeddingLoading.load([glove_dim])\n",
    "max_tokens = 18 # (determined empirically)\n",
    "\n",
    "word_level_glove_embs = torch.zeros(n, max_tokens, glove_dim)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "for i in range(n):\n",
    "    tokenization = word_tokenize(headlines1[i])\n",
    "    embs = []\n",
    "    for token in tokenization:\n",
    "        emb = glove.get(token)\n",
    "        if emb is None:\n",
    "            continue\n",
    "        embs.append(emb)\n",
    "    embs = torch.tensor(embs)\n",
    "    word_level_glove_embs[i,:embs.shape[0],:] = embs\n",
    "    X3[i] = torch.mean(embs, 0)\n",
    "\n",
    "torch.save(word_level_glove_embs, 'word-level_glove_embeddings.pt')\n",
    "\n",
    "print('Saved word-level glove embeddings')\n",
    "print('Created averaged glove embeddings')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved word-level glove embeddings\n",
      "Created averaged glove embeddings\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# doc2vec_dim = int(mean_words_per_headline)\n",
    "# doc2vec_dim = 4\n",
    "dm_doc2vec_dim = 4\n",
    "dbow_doc2vec_dim = 4\n",
    "\n",
    "documents = [TaggedDocument(doc,[i]) for i, doc in enumerate(headlines_words)]\n",
    "\n",
    "dm_model = Doc2Vec(documents, dm=1, vector_size=dm_doc2vec_dim,\n",
    "                window=4, min_count=1, seed=1, workers=1)\n",
    "dbow_model = Doc2Vec(documents, dm=0, vector_size=dbow_doc2vec_dim,\n",
    "                     window=4, min_count=1, seed=1, workers=1)\n",
    "\n",
    "# Parameters:\n",
    "# dm ({1,0}, optional) – Defines the training algorithm. If dm=1, ‘distributed memory’\n",
    "#  (PV-DM) is used. Otherwise, distributed bag of words (PV-DBOW) is employed.\n",
    "# vector_size (int, optional) – Dimensionality of the feature vectors.\n",
    "# window (int, optional) – The maximum distance between the current and predicted word\n",
    "#   within a sentence.\n",
    "# min_count (int, optional) – Ignores all words with total frequency lower than this.\n",
    "# seed (int, optional) – Seed for the random number generator. Initial vectors for each\n",
    "#   word are seeded with a hash of the concatenation of word + str(seed). Note that for\n",
    "#   a fully deterministically-reproducible run, you must also limit the model to a\n",
    "#   single worker thread (workers=1), to eliminate ordering jitter from OS thread\n",
    "#   scheduling. In Python 3, reproducibility between interpreter launches also\n",
    "#   requires use of the PYTHONHASHSEED environment variable to control hash randomization.\n",
    "# workers (int, optional) – Use these many worker threads to train the model\n",
    "#   (=faster training with multicore machines).\n",
    "\n",
    "X4 = torch.tensor(np.stack([dm_model.dv.get_vector(i, norm=True) for i in range(n)], 0))\n",
    "print('Created distributed memory Doc2Vec embeddings')\n",
    "\n",
    "X5 = torch.tensor(np.stack([dbow_model.dv.get_vector(i, norm=True) for i in range(n)], 0))\n",
    "print('Created distributed bag of words Doc2Vec embeddings')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created distributed memory Doc2Vec embeddings\n",
      "Created distributed bag of words Doc2Vec embeddings\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "X4_3_to_12 = torch.zeros(10,1250,12)\n",
    "X5_3_to_12 = torch.zeros(10,1250,12)\n",
    "\n",
    "# use for generating doc2vecs of differing sizes\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "documents = [TaggedDocument(doc,[i]) for i, doc in enumerate(headlines_words)]\n",
    "\n",
    "for doc2vec_dim in range(3,13):\n",
    "\n",
    "    dm_model = Doc2Vec(documents, dm=1, vector_size=doc2vec_dim,\n",
    "                    window=4, min_count=1, seed=1, workers=1)\n",
    "    dbow_model = Doc2Vec(documents, dm=0, vector_size=doc2vec_dim,\n",
    "                         window=4, min_count=1, seed=1, workers=1)\n",
    "\n",
    "    X4_3_to_12[doc2vec_dim-3,:,:doc2vec_dim] = torch.tensor(np.stack([dm_model.dv.get_vector(i, norm=True) for i in range(n)], 0))\n",
    "\n",
    "    X5_3_to_12[doc2vec_dim-3,:,:doc2vec_dim] = torch.tensor(np.stack([dbow_model.dv.get_vector(i, norm=True) for i in range(n)], 0))\n",
    "\n",
    "torch.save(X4_3_to_12, 'X4_3_to_12_tensor')\n",
    "torch.save(X5_3_to_12, 'X5_3_to_12_tensor')\n",
    "\n",
    "# 4, 7, 9, 11 are top performers for both models\n",
    "# size 4 seems to do the best for both models\n",
    "# (refer to mogp_testing.jpyng)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1250, 162])\n"
     ]
    }
   ],
   "source": [
    "X = torch.cat((X0,X1,X2,X3,X4,X5),1)\n",
    "print(X.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor saved to 'input_tensor.pt'\n"
     ]
    }
   ],
   "source": [
    "torch.save(X, 'input_tensor.pt')\n",
    "print(\"Tensor saved to \\'input_tensor.pt\\'\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To be used in notebooks that utilize this one:\n",
    "\n",
    "```\n",
    "doc2vec_size = 4\n",
    "X = torch.load('input_tensor.pt')\n",
    "sizes = (1,88,15,50,doc2vec_size,doc2vec_size)\n",
    "_distinct_embs = len(sizes)\n",
    "_breaks = [sum(sizes[:i]) for i in range(_distinct_embs+1)]\n",
    "indices = ((_breaks[i],_breaks[i+1]) for i in range(_distinct_embs))\n",
    "(X0,X1,X2,X3,X4,X5) = (X[:,slice(*i)] for i in indices)\n",
    "\n",
    "separation = 250\n",
    "X_train, X_test = X[:separation], X[separation:]\n",
    "Y_train, Y_test = Y[:separation], Y[separation:]\n",
    "```\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}